{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "580f52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e03317d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'mates',\n",
    "    'user': 'postgres',\n",
    "    'password': '1',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "CONNECTION_STRING = \"postgresql://postgres:1@localhost:5432/mates\"\n",
    "JSON_FILE_PATH = r\"d:\\Menghour\\MATES\\scraping\\notebook\\khmer_news_formatted.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5781d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized database tables\n",
    "def create_normalized_tables(conn):\n",
    "    \"\"\"Create all normalized tables\"\"\"\n",
    "    create_tables_sql = \"\"\"\n",
    "    -- Drop tables if they exist (for clean setup)\n",
    "    DROP TABLE IF EXISTS article_tags CASCADE;\n",
    "    DROP TABLE IF EXISTS articles CASCADE;\n",
    "    DROP TABLE IF EXISTS tags CASCADE;\n",
    "    DROP TABLE IF EXISTS sources CASCADE;\n",
    "    DROP TABLE IF EXISTS categories CASCADE;\n",
    "\n",
    "    -- Create categories table\n",
    "    CREATE TABLE categories (\n",
    "        category_id SERIAL PRIMARY KEY,\n",
    "        category_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "        description TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    -- Create sources table\n",
    "    CREATE TABLE sources (\n",
    "        source_id SERIAL PRIMARY KEY,\n",
    "        source_url VARCHAR(500) UNIQUE NOT NULL,\n",
    "        source_name VARCHAR(200) NOT NULL,\n",
    "        is_active BOOLEAN DEFAULT TRUE,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    -- Create tags table\n",
    "    CREATE TABLE tags (\n",
    "        tag_id SERIAL PRIMARY KEY,\n",
    "        tag_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    -- Create articles table\n",
    "    CREATE TABLE articles (\n",
    "        article_id SERIAL PRIMARY KEY,\n",
    "        url VARCHAR(1000) UNIQUE NOT NULL,\n",
    "        source_id INTEGER REFERENCES sources(source_id),\n",
    "        publication_date DATE NOT NULL,\n",
    "        scrape_date TIMESTAMP NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        content TEXT,\n",
    "        word_count INTEGER DEFAULT 0,\n",
    "        sentence_count INTEGER DEFAULT 0,\n",
    "        character_count INTEGER DEFAULT 0,\n",
    "        category_id INTEGER REFERENCES categories(category_id),\n",
    "        category_confidence DECIMAL(3,2) DEFAULT 0.0,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    -- Create article_tags junction table\n",
    "    CREATE TABLE article_tags (\n",
    "        article_tag_id SERIAL PRIMARY KEY,\n",
    "        article_id INTEGER REFERENCES articles(article_id) ON DELETE CASCADE,\n",
    "        tag_id INTEGER REFERENCES tags(tag_id) ON DELETE CASCADE,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE(article_id, tag_id)\n",
    "    );\n",
    "\n",
    "    -- Create indexes for better performance\n",
    "    CREATE INDEX idx_articles_url ON articles(url);\n",
    "    CREATE INDEX idx_articles_publication_date ON articles(publication_date);\n",
    "    CREATE INDEX idx_articles_source_id ON articles(source_id);\n",
    "    CREATE INDEX idx_articles_category_id ON articles(category_id);\n",
    "    CREATE INDEX idx_articles_created_at ON articles(created_at);\n",
    "    CREATE INDEX idx_article_tags_article_id ON article_tags(article_id);\n",
    "    CREATE INDEX idx_article_tags_tag_id ON article_tags(tag_id);\n",
    "    CREATE INDEX idx_sources_url ON sources(source_url);\n",
    "    CREATE INDEX idx_categories_name ON categories(category_name);\n",
    "    CREATE INDEX idx_tags_name ON tags(tag_name);\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(create_tables_sql)\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        print(\"Normalized tables created successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Table creation failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebdf4bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successful!\n",
      "Normalized tables created successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test database connection and create tables\n",
    "def test_connection():\n",
    "    \"\"\"Test the database connection\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        print(\"Database connection successful!\")\n",
    "        \n",
    "        # Create normalized tables\n",
    "        create_normalized_tables(conn)\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Database connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c9bbe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 7856 articles\n",
      "\n",
      "Sample of first article:\n",
      "Title: ចំនួននៃការវាយប្រហារ ទៅលើពលរដ្ឋរុស្ស៊ី នៅក្នុងប្រទេសអាល្លឺម៉ង់ កើនឡើងជារៀងរាល់ថ្ងៃ...\n",
      "URL: https://dap-news.com/international/2022/03/29/232389/\n",
      "Source: https://dap-news.com\n",
      "Publication Date: 03/29/2022\n",
      "Failed to load JSON file: 'primary_category'\n"
     ]
    }
   ],
   "source": [
    "# Load and preview JSON data\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Load and preview JSON data\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(data)} articles\")\n",
    "        print(\"\\nSample of first article:\")\n",
    "        print(f\"Title: {data[0]['title'][:100]}...\")\n",
    "        print(f\"URL: {data[0]['url']}\")\n",
    "        print(f\"Source: {data[0]['source']}\")\n",
    "        print(f\"Publication Date: {data[0]['publication_date']}\")\n",
    "        print(f\"Primary Category: {data[0]['primary_category']}\")\n",
    "        print(f\"Tags: {data[0]['tags'][:5]}\")  # First 5 tags\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load JSON file: {e}\")\n",
    "        return []\n",
    "    \n",
    "# Load your data\n",
    "articles_data = load_json_data(JSON_FILE_PATH)\n",
    "\n",
    "# Show basic statistics\n",
    "if articles_data:\n",
    "    print(f\"\\nData Overview:\")\n",
    "    print(f\"Total articles: {len(articles_data)}\")\n",
    "    print(f\"Unique sources: {len(set(article['source'] for article in articles_data))}\")\n",
    "    print(f\"Unique categories: {len(set(article['primary_category'] for article in articles_data))}\")\n",
    "    print(f\"Total unique tags: {len(set(tag for article in articles_data for tag in article.get('tags', [])))}\")\n",
    "    print(f\"Date range: {min(article['publication_date'] for article in articles_data)} to {max(article['publication_date'] for article in articles_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4430865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Normalized ETL Process...\n",
      "Cache loaded successfully\n",
      "Step 1: Extracting data from JSON...\n",
      "Loaded 7856 articles\n",
      "Step 2: Transforming data...\n",
      "Successfully transformed: 7856\n",
      "Failed transformations: 0\n",
      "Step 3: Loading data to normalized tables...\n",
      "Batch 1/158: 50 successful, 0 failed\n",
      "Batch 2/158: 50 successful, 0 failed\n",
      "Batch 3/158: 50 successful, 0 failed\n",
      "Batch 4/158: 50 successful, 0 failed\n",
      "Batch 5/158: 50 successful, 0 failed\n",
      "Batch 6/158: 50 successful, 0 failed\n",
      "Batch 7/158: 50 successful, 0 failed\n",
      "Batch 8/158: 50 successful, 0 failed\n",
      "Batch 9/158: 50 successful, 0 failed\n",
      "Batch 10/158: 50 successful, 0 failed\n",
      "Batch 11/158: 50 successful, 0 failed\n",
      "Batch 12/158: 50 successful, 0 failed\n",
      "Batch 13/158: 50 successful, 0 failed\n",
      "Batch 14/158: 50 successful, 0 failed\n",
      "Batch 15/158: 50 successful, 0 failed\n",
      "Batch 16/158: 50 successful, 0 failed\n",
      "Batch 17/158: 50 successful, 0 failed\n",
      "Batch 18/158: 50 successful, 0 failed\n",
      "Batch 19/158: 50 successful, 0 failed\n",
      "Batch 20/158: 50 successful, 0 failed\n",
      "Batch 21/158: 50 successful, 0 failed\n",
      "Batch 22/158: 50 successful, 0 failed\n",
      "Batch 23/158: 50 successful, 0 failed\n",
      "Batch 24/158: 50 successful, 0 failed\n",
      "Batch 25/158: 50 successful, 0 failed\n",
      "Batch 26/158: 50 successful, 0 failed\n",
      "Batch 27/158: 50 successful, 0 failed\n",
      "Batch 28/158: 50 successful, 0 failed\n",
      "Batch 29/158: 50 successful, 0 failed\n",
      "Batch 30/158: 50 successful, 0 failed\n",
      "Batch 31/158: 50 successful, 0 failed\n",
      "Batch 32/158: 50 successful, 0 failed\n",
      "Batch 33/158: 50 successful, 0 failed\n",
      "Batch 34/158: 50 successful, 0 failed\n",
      "Batch 35/158: 50 successful, 0 failed\n",
      "Batch 36/158: 50 successful, 0 failed\n",
      "Batch 37/158: 50 successful, 0 failed\n",
      "Batch 38/158: 50 successful, 0 failed\n",
      "Batch 39/158: 50 successful, 0 failed\n",
      "Batch 40/158: 50 successful, 0 failed\n",
      "Batch 41/158: 50 successful, 0 failed\n",
      "Batch 42/158: 50 successful, 0 failed\n",
      "Batch 43/158: 50 successful, 0 failed\n",
      "Batch 44/158: 50 successful, 0 failed\n",
      "Batch 45/158: 50 successful, 0 failed\n",
      "Batch 46/158: 50 successful, 0 failed\n",
      "Batch 47/158: 50 successful, 0 failed\n",
      "Batch 48/158: 50 successful, 0 failed\n",
      "Batch 49/158: 50 successful, 0 failed\n",
      "Batch 50/158: 50 successful, 0 failed\n",
      "Batch 51/158: 50 successful, 0 failed\n",
      "Batch 52/158: 50 successful, 0 failed\n",
      "Batch 53/158: 50 successful, 0 failed\n",
      "Batch 54/158: 50 successful, 0 failed\n",
      "Batch 55/158: 50 successful, 0 failed\n",
      "Batch 56/158: 50 successful, 0 failed\n",
      "Batch 57/158: 50 successful, 0 failed\n",
      "Batch 58/158: 50 successful, 0 failed\n",
      "Batch 59/158: 50 successful, 0 failed\n",
      "Batch 60/158: 50 successful, 0 failed\n",
      "Batch 61/158: 50 successful, 0 failed\n",
      "Batch 62/158: 50 successful, 0 failed\n",
      "Batch 63/158: 50 successful, 0 failed\n",
      "Batch 64/158: 50 successful, 0 failed\n",
      "Batch 65/158: 50 successful, 0 failed\n",
      "Batch 66/158: 50 successful, 0 failed\n",
      "Batch 67/158: 50 successful, 0 failed\n",
      "Batch 68/158: 50 successful, 0 failed\n",
      "Batch 69/158: 50 successful, 0 failed\n",
      "Batch 70/158: 50 successful, 0 failed\n",
      "Batch 71/158: 50 successful, 0 failed\n",
      "Batch 72/158: 50 successful, 0 failed\n",
      "Batch 73/158: 50 successful, 0 failed\n",
      "Batch 74/158: 50 successful, 0 failed\n",
      "Batch 75/158: 50 successful, 0 failed\n",
      "Batch 76/158: 50 successful, 0 failed\n",
      "Batch 77/158: 50 successful, 0 failed\n",
      "Batch 78/158: 50 successful, 0 failed\n",
      "Batch 79/158: 50 successful, 0 failed\n",
      "Batch 80/158: 50 successful, 0 failed\n",
      "Batch 81/158: 50 successful, 0 failed\n",
      "Batch 82/158: 50 successful, 0 failed\n",
      "Batch 83/158: 50 successful, 0 failed\n",
      "Batch 84/158: 50 successful, 0 failed\n",
      "Batch 85/158: 50 successful, 0 failed\n",
      "Batch 86/158: 50 successful, 0 failed\n",
      "Batch 87/158: 50 successful, 0 failed\n",
      "Batch 88/158: 50 successful, 0 failed\n",
      "Batch 89/158: 50 successful, 0 failed\n",
      "Batch 90/158: 50 successful, 0 failed\n",
      "Batch 91/158: 50 successful, 0 failed\n",
      "Batch 92/158: 50 successful, 0 failed\n",
      "Batch 93/158: 50 successful, 0 failed\n",
      "Batch 94/158: 50 successful, 0 failed\n",
      "Batch 95/158: 50 successful, 0 failed\n",
      "Batch 96/158: 50 successful, 0 failed\n",
      "Batch 97/158: 50 successful, 0 failed\n",
      "Batch 98/158: 50 successful, 0 failed\n",
      "Batch 99/158: 50 successful, 0 failed\n",
      "Batch 100/158: 50 successful, 0 failed\n",
      "Batch 101/158: 50 successful, 0 failed\n",
      "Batch 102/158: 50 successful, 0 failed\n",
      "Batch 103/158: 50 successful, 0 failed\n",
      "Batch 104/158: 50 successful, 0 failed\n",
      "Batch 105/158: 50 successful, 0 failed\n",
      "Batch 106/158: 50 successful, 0 failed\n",
      "Batch 107/158: 50 successful, 0 failed\n",
      "Batch 108/158: 50 successful, 0 failed\n",
      "Batch 109/158: 50 successful, 0 failed\n",
      "Batch 110/158: 50 successful, 0 failed\n",
      "Batch 111/158: 50 successful, 0 failed\n",
      "Batch 112/158: 50 successful, 0 failed\n",
      "Batch 113/158: 50 successful, 0 failed\n",
      "Batch 114/158: 50 successful, 0 failed\n",
      "Batch 115/158: 50 successful, 0 failed\n",
      "Batch 116/158: 50 successful, 0 failed\n",
      "Batch 117/158: 50 successful, 0 failed\n",
      "Batch 118/158: 50 successful, 0 failed\n",
      "Batch 119/158: 50 successful, 0 failed\n",
      "Batch 120/158: 50 successful, 0 failed\n",
      "Batch 121/158: 50 successful, 0 failed\n",
      "Batch 122/158: 50 successful, 0 failed\n",
      "Batch 123/158: 50 successful, 0 failed\n",
      "Batch 124/158: 50 successful, 0 failed\n",
      "Batch 125/158: 50 successful, 0 failed\n",
      "Batch 126/158: 50 successful, 0 failed\n",
      "Batch 127/158: 50 successful, 0 failed\n",
      "Batch 128/158: 50 successful, 0 failed\n",
      "Batch 129/158: 50 successful, 0 failed\n",
      "Batch 130/158: 50 successful, 0 failed\n",
      "Batch 131/158: 50 successful, 0 failed\n",
      "Batch 132/158: 50 successful, 0 failed\n",
      "Batch 133/158: 50 successful, 0 failed\n",
      "Batch 134/158: 50 successful, 0 failed\n",
      "Batch 135/158: 50 successful, 0 failed\n",
      "Batch 136/158: 50 successful, 0 failed\n",
      "Batch 137/158: 50 successful, 0 failed\n",
      "Batch 138/158: 50 successful, 0 failed\n",
      "Batch 139/158: 50 successful, 0 failed\n",
      "Batch 140/158: 50 successful, 0 failed\n",
      "Batch 141/158: 50 successful, 0 failed\n",
      "Batch 142/158: 50 successful, 0 failed\n",
      "Batch 143/158: 50 successful, 0 failed\n",
      "Batch 144/158: 50 successful, 0 failed\n",
      "Batch 145/158: 50 successful, 0 failed\n",
      "Batch 146/158: 50 successful, 0 failed\n",
      "Batch 147/158: 50 successful, 0 failed\n",
      "Batch 148/158: 50 successful, 0 failed\n",
      "Batch 149/158: 50 successful, 0 failed\n",
      "Batch 150/158: 50 successful, 0 failed\n",
      "Batch 151/158: 50 successful, 0 failed\n",
      "Batch 152/158: 50 successful, 0 failed\n",
      "Batch 153/158: 50 successful, 0 failed\n",
      "Batch 154/158: 50 successful, 0 failed\n",
      "Batch 155/158: 50 successful, 0 failed\n",
      "Batch 156/158: 50 successful, 0 failed\n",
      "Batch 157/158: 50 successful, 0 failed\n",
      "Batch 158/158: 6 successful, 0 failed\n",
      "ETL Complete!\n",
      "Final Results: 7856 successful, 0 failed\n"
     ]
    }
   ],
   "source": [
    "# ETL Processor for Normalized Database\n",
    "class NormalizedETL:\n",
    "    def __init__(self, db_config):\n",
    "        self.db_config = db_config\n",
    "        self.conn = None\n",
    "        self.cache = {\n",
    "            'sources': {},\n",
    "            'categories': {},\n",
    "            'tags': {}\n",
    "        }\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(**self.db_config)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Database connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing data into cache to avoid duplicates\"\"\"\n",
    "        try:\n",
    "            cur = self.conn.cursor()\n",
    "            \n",
    "            # Load sources\n",
    "            cur.execute(\"SELECT source_id, source_url FROM sources\")\n",
    "            for source_id, source_url in cur.fetchall():\n",
    "                self.cache['sources'][source_url] = source_id\n",
    "            \n",
    "            # Load categories\n",
    "            cur.execute(\"SELECT category_id, category_name FROM categories\")\n",
    "            for category_id, category_name in cur.fetchall():\n",
    "                self.cache['categories'][category_name] = category_id\n",
    "            \n",
    "            # Load tags\n",
    "            cur.execute(\"SELECT tag_id, tag_name FROM tags\")\n",
    "            for tag_id, tag_name in cur.fetchall():\n",
    "                self.cache['tags'][tag_name] = tag_id\n",
    "            \n",
    "            cur.close()\n",
    "            print(\"Cache loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cache loading failed: {e}\")\n",
    "    \n",
    "    def get_or_create_source(self, source_url, source_name):\n",
    "        \"\"\"Get existing source ID or create new source\"\"\"\n",
    "        if source_url in self.cache['sources']:\n",
    "            return self.cache['sources'][source_url]\n",
    "        \n",
    "        try:\n",
    "            cur = self.conn.cursor()\n",
    "            cur.execute(\n",
    "                \"INSERT INTO sources (source_url, source_name) VALUES (%s, %s) RETURNING source_id\",\n",
    "                (source_url, source_name)\n",
    "            )\n",
    "            source_id = cur.fetchone()[0]\n",
    "            self.conn.commit()\n",
    "            self.cache['sources'][source_url] = source_id\n",
    "            cur.close()\n",
    "            return source_id\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create source {source_url}: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None\n",
    "    \n",
    "    def get_or_create_category(self, category_name):\n",
    "        \"\"\"Get existing category ID or create new category\"\"\"\n",
    "        if category_name in self.cache['categories']:\n",
    "            return self.cache['categories'][category_name]\n",
    "        \n",
    "        try:\n",
    "            cur = self.conn.cursor()\n",
    "            cur.execute(\n",
    "                \"INSERT INTO categories (category_name) VALUES (%s) RETURNING category_id\",\n",
    "                (category_name,)\n",
    "            )\n",
    "            category_id = cur.fetchone()[0]\n",
    "            self.conn.commit()\n",
    "            self.cache['categories'][category_name] = category_id\n",
    "            cur.close()\n",
    "            return category_id\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create category {category_name}: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None\n",
    "    \n",
    "    def get_or_create_tag(self, tag_name):\n",
    "        \"\"\"Get existing tag ID or create new tag\"\"\"\n",
    "        if tag_name in self.cache['tags']:\n",
    "            return self.cache['tags'][tag_name]\n",
    "        \n",
    "        try:\n",
    "            cur = self.conn.cursor()\n",
    "            cur.execute(\n",
    "                \"INSERT INTO tags (tag_name) VALUES (%s) RETURNING tag_id\",\n",
    "                (tag_name,)\n",
    "            )\n",
    "            tag_id = cur.fetchone()[0]\n",
    "            self.conn.commit()\n",
    "            self.cache['tags'][tag_name] = tag_id\n",
    "            cur.close()\n",
    "            return tag_id\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create tag {tag_name}: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None\n",
    "    \n",
    "    def transform_article(self, article):\n",
    "        \"\"\"Transform a single article\"\"\"\n",
    "        try:\n",
    "            # Convert dates\n",
    "            publication_date = datetime.strptime(article['publication_date'], '%m/%d/%Y').date()\n",
    "            scrape_date = datetime.strptime(article['scrape_date'], '%m/%d/%Y %H:%M')\n",
    "            \n",
    "            transformed = {\n",
    "                'url': article['url'],\n",
    "                'source_url': article['source'],\n",
    "                'source_name': article['source'].replace('https://', '').replace('http://', '').split('/')[0],\n",
    "                'publication_date': publication_date,\n",
    "                'scrape_date': scrape_date,\n",
    "                'title': article['title'],\n",
    "                'content': article.get('content', ''),\n",
    "                'tags': article.get('tags', []),\n",
    "                'word_count': article.get('word_count', 0),\n",
    "                'sentence_count': article.get('sentence_count', 0),\n",
    "                'character_count': article.get('character_count', 0),\n",
    "                'primary_category': article.get('primary_category', 'unknown'),\n",
    "                'category_confidence': float(article.get('category_confidence', 0.0))\n",
    "            }\n",
    "            \n",
    "            return transformed\n",
    "        except Exception as e:\n",
    "            print(f\"Transformation failed for {article.get('url', 'unknown')}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_article(self, transformed_article):\n",
    "        \"\"\"Process a single article into normalized tables\"\"\"\n",
    "        try:\n",
    "            cur = self.conn.cursor()\n",
    "            \n",
    "            # Get or create source\n",
    "            source_id = self.get_or_create_source(\n",
    "                transformed_article['source_url'],\n",
    "                transformed_article['source_name']\n",
    "            )\n",
    "            if not source_id:\n",
    "                return False\n",
    "            \n",
    "            # Get or create category\n",
    "            category_id = self.get_or_create_category(transformed_article['primary_category'])\n",
    "            if not category_id:\n",
    "                return False\n",
    "            \n",
    "            # Insert article\n",
    "            article_query = \"\"\"\n",
    "            INSERT INTO articles (\n",
    "                url, source_id, publication_date, scrape_date, title, content,\n",
    "                word_count, sentence_count, character_count, category_id, category_confidence\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (url) DO UPDATE SET\n",
    "                title = EXCLUDED.title,\n",
    "                content = EXCLUDED.content,\n",
    "                word_count = EXCLUDED.word_count,\n",
    "                category_id = EXCLUDED.category_id,\n",
    "                category_confidence = EXCLUDED.category_confidence\n",
    "            RETURNING article_id\n",
    "            \"\"\"\n",
    "            \n",
    "            cur.execute(article_query, (\n",
    "                transformed_article['url'],\n",
    "                source_id,\n",
    "                transformed_article['publication_date'],\n",
    "                transformed_article['scrape_date'],\n",
    "                transformed_article['title'],\n",
    "                transformed_article['content'],\n",
    "                transformed_article['word_count'],\n",
    "                transformed_article['sentence_count'],\n",
    "                transformed_article['character_count'],\n",
    "                category_id,\n",
    "                transformed_article['category_confidence']\n",
    "            ))\n",
    "            \n",
    "            article_id = cur.fetchone()[0]\n",
    "            \n",
    "            # Process tags\n",
    "            for tag_name in transformed_article['tags']:\n",
    "                tag_id = self.get_or_create_tag(tag_name)\n",
    "                if tag_id:\n",
    "                    try:\n",
    "                        cur.execute(\n",
    "                            \"INSERT INTO article_tags (article_id, tag_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                            (article_id, tag_id)\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to link tag {tag_name} to article: {e}\")\n",
    "            \n",
    "            self.conn.commit()\n",
    "            cur.close()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process article {transformed_article['url']}: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "    \n",
    "    def run_etl(self, json_file_path, batch_size=50):\n",
    "        \"\"\"Run the complete ETL process\"\"\"\n",
    "        print(\"Starting Normalized ETL Process...\")\n",
    "        \n",
    "        if not self.connect():\n",
    "            return\n",
    "        \n",
    "        # Load cache of existing data\n",
    "        self.load_cache()\n",
    "        \n",
    "        # Step 1: Extract\n",
    "        print(\"Step 1: Extracting data from JSON...\")\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            raw_articles = json.load(file)\n",
    "        \n",
    "        print(f\"Loaded {len(raw_articles)} articles\")\n",
    "        \n",
    "        # Step 2: Transform\n",
    "        print(\"Step 2: Transforming data...\")\n",
    "        transformed_articles = []\n",
    "        failed_transformations = 0\n",
    "        \n",
    "        for article in raw_articles:\n",
    "            transformed = self.transform_article(article)\n",
    "            if transformed:\n",
    "                transformed_articles.append(transformed)\n",
    "            else:\n",
    "                failed_transformations += 1\n",
    "        \n",
    "        print(f\"Successfully transformed: {len(transformed_articles)}\")\n",
    "        print(f\"Failed transformations: {failed_transformations}\")\n",
    "        \n",
    "        # Step 3: Load\n",
    "        print(\"Step 3: Loading data to normalized tables...\")\n",
    "        \n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(transformed_articles), batch_size):\n",
    "            batch = transformed_articles[i:i + batch_size]\n",
    "            batch_success = 0\n",
    "            batch_error = 0\n",
    "            \n",
    "            for article in batch:\n",
    "                if self.process_article(article):\n",
    "                    batch_success += 1\n",
    "                else:\n",
    "                    batch_error += 1\n",
    "            \n",
    "            success_count += batch_success\n",
    "            error_count += batch_error\n",
    "            \n",
    "            print(f\"Batch {i//batch_size + 1}/{(len(transformed_articles) + batch_size - 1)//batch_size}: {batch_success} successful, {batch_error} failed\")\n",
    "        \n",
    "        self.conn.close()\n",
    "        \n",
    "        print(f\"ETL Complete!\")\n",
    "        print(f\"Final Results: {success_count} successful, {error_count} failed\")\n",
    "\n",
    "# Run the normalized ETL process\n",
    "etl = NormalizedETL(DB_CONFIG)\n",
    "etl.run_etl(JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bcac722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Complete!\n",
      "\n",
      "Table Statistics:\n",
      "   Articles: 7856\n",
      "   Sources: 1\n",
      "   Categories: 1\n",
      "   Tags: 0\n",
      "   Article-Tag relationships: 0\n",
      "\n",
      "Latest 5 articles:\n",
      "   1. របស់រុស្ស៊ីថា ចារកម្មអ៊ុយក្រែន និងអង់គ្លេស ព្យាយាមសូកអ្នក បើ...\n",
      "      Source: dap-news.com | Category: unknown | Date: 2025-11-11\n",
      "   2. មេអាវុធហត្ថខេត្តកំពង់ស្ពឺ ជម្រុញឱ្យមន្ត្រីជំនាញបង្កើនការចុះផ...\n",
      "      Source: dap-news.com | Category: unknown | Date: 2025-11-11\n",
      "   3. មន្ទីរអភិវឌ្ឍន៍ជនបទ ប្រគល់ភារកិច្ចជូនអាជ្ញាធរភូមិឃុំនិងស្រុក...\n",
      "      Source: dap-news.com | Category: unknown | Date: 2025-11-11\n",
      "   4. លោក ខូយ រីដា អញ្ជើញចុះពិនិត្យ វឌ្ឍនភាពការងារសាង់សង់គម្រោងវារ...\n",
      "      Source: dap-news.com | Category: unknown | Date: 2025-11-11\n",
      "   5. លោកឧត្តមសេនីយ៍ត្រី ម៉េង ស្រ៊ុន មេបញ្ជាការ បានអញ្ជើញជាអធិបតីក...\n",
      "      Source: dap-news.com | Category: unknown | Date: 2025-11-11\n",
      "\n",
      "Category distribution:\n",
      "   unknown: 7856 articles\n",
      "\n",
      "Top 10 tags:\n"
     ]
    }
   ],
   "source": [
    "# Verify the data was loaded into normalized tables\n",
    "def verify_normalized_data():\n",
    "    \"\"\"Verify that data was successfully loaded into normalized tables\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Count records in each table\n",
    "        cur.execute(\"SELECT COUNT(*) FROM articles\")\n",
    "        article_count = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM sources\")\n",
    "        source_count = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM categories\")\n",
    "        category_count = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM tags\")\n",
    "        tag_count = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM article_tags\")\n",
    "        article_tag_count = cur.fetchone()[0]\n",
    "        \n",
    "        # Get some sample data with joins\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT a.title, s.source_name, c.category_name, a.publication_date\n",
    "            FROM articles a\n",
    "            JOIN sources s ON a.source_id = s.source_id\n",
    "            JOIN categories c ON a.category_id = c.category_id\n",
    "            ORDER BY a.publication_date DESC \n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        sample_articles = cur.fetchall()\n",
    "        \n",
    "        # Get category distribution\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT c.category_name, COUNT(*) as article_count\n",
    "            FROM articles a\n",
    "            JOIN categories c ON a.category_id = c.category_id\n",
    "            GROUP BY c.category_name \n",
    "            ORDER BY article_count DESC\n",
    "        \"\"\")\n",
    "        category_stats = cur.fetchall()\n",
    "        \n",
    "        # Get top tags\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT t.tag_name, COUNT(*) as usage_count\n",
    "            FROM article_tags at\n",
    "            JOIN tags t ON at.tag_id = t.tag_id\n",
    "            GROUP BY t.tag_name\n",
    "            ORDER BY usage_count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        top_tags = cur.fetchall()\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Verification Complete!\")\n",
    "        print(f\"\\nTable Statistics:\")\n",
    "        print(f\"   Articles: {article_count}\")\n",
    "        print(f\"   Sources: {source_count}\")\n",
    "        print(f\"   Categories: {category_count}\")\n",
    "        print(f\"   Tags: {tag_count}\")\n",
    "        print(f\"   Article-Tag relationships: {article_tag_count}\")\n",
    "        \n",
    "        print(f\"\\nLatest 5 articles:\")\n",
    "        for i, (title, source, category, date) in enumerate(sample_articles, 1):\n",
    "            print(f\"   {i}. {title[:60]}...\")\n",
    "            print(f\"      Source: {source} | Category: {category} | Date: {date}\")\n",
    "        \n",
    "        print(f\"\\nCategory distribution:\")\n",
    "        for category, count in category_stats:\n",
    "            print(f\"   {category}: {count} articles\")\n",
    "        \n",
    "        print(f\"\\nTop 10 tags:\")\n",
    "        for tag, count in top_tags:\n",
    "            print(f\"   {tag}: {count} articles\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Verification failed: {e}\")\n",
    "\n",
    "# Run verification\n",
    "verify_normalized_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0032e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZED DATA DASHBOARD\n",
      "============================================================\n",
      "Overall Statistics:\n",
      "   Total Articles: 7856\n",
      "   Unique Sources: 1\n",
      "   Unique Categories: 1\n",
      "   Unique Tags: 0\n",
      "   Date Range: 2019-02-15 to 2025-11-11\n",
      "\n",
      "Category Distribution:\n",
      "   unknown: 7856 articles (100.0%)\n",
      "\n",
      "Source Distribution:\n",
      "   dap-news.com: 7856 articles (100.0%)\n",
      "\n",
      "Content Statistics:\n",
      "   Average word count: 0\n",
      "   Min word count: 0\n",
      "   Max word count: 0\n",
      "\n",
      "Top 10 Most Used Tags:\n",
      "\n",
      "Latest Articles:\n",
      "Dashboard creation failed: Column 'publication_date' has dtype object, cannot use method 'nlargest' with this dtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eii Lingg\\AppData\\Local\\Temp\\ipykernel_14440\\1630780725.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles_df = pd.read_sql(\"\"\"\n",
      "C:\\Users\\Eii Lingg\\AppData\\Local\\Temp\\ipykernel_14440\\1630780725.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  tags_df = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive dashboard\n",
    "def create_normalized_dashboard():\n",
    "    \"\"\"Create a comprehensive dashboard showing normalized data\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        \n",
    "        # Load data into pandas DataFrames\n",
    "        articles_df = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                a.title,\n",
    "                s.source_name,\n",
    "                c.category_name,\n",
    "                a.publication_date,\n",
    "                a.word_count,\n",
    "                a.scrape_date\n",
    "            FROM articles a\n",
    "            JOIN sources s ON a.source_id = s.source_id\n",
    "            JOIN categories c ON a.category_id = c.category_id\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        tags_df = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                t.tag_name,\n",
    "                COUNT(at.article_id) as article_count\n",
    "            FROM tags t\n",
    "            LEFT JOIN article_tags at ON t.tag_id = at.tag_id\n",
    "            GROUP BY t.tag_name\n",
    "            ORDER BY article_count DESC\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        print(\"NORMALIZED DATA DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"Overall Statistics:\")\n",
    "        print(f\"   Total Articles: {len(articles_df)}\")\n",
    "        print(f\"   Unique Sources: {articles_df['source_name'].nunique()}\")\n",
    "        print(f\"   Unique Categories: {articles_df['category_name'].nunique()}\")\n",
    "        print(f\"   Unique Tags: {len(tags_df)}\")\n",
    "        print(f\"   Date Range: {articles_df['publication_date'].min()} to {articles_df['publication_date'].max()}\")\n",
    "        \n",
    "        print(f\"\\nCategory Distribution:\")\n",
    "        category_counts = articles_df['category_name'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            percentage = (count / len(articles_df)) * 100\n",
    "            print(f\"   {category}: {count} articles ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSource Distribution:\")\n",
    "        source_counts = articles_df['source_name'].value_counts().head(5)\n",
    "        for source, count in source_counts.items():\n",
    "            percentage = (count / len(articles_df)) * 100\n",
    "            print(f\"   {source}: {count} articles ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nContent Statistics:\")\n",
    "        print(f\"   Average word count: {articles_df['word_count'].mean():.0f}\")\n",
    "        print(f\"   Min word count: {articles_df['word_count'].min()}\")\n",
    "        print(f\"   Max word count: {articles_df['word_count'].max()}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Used Tags:\")\n",
    "        for i, (_, row) in enumerate(tags_df.head(10).iterrows(), 1):\n",
    "            print(f\"   {i}. {row['tag_name']}: {row['article_count']} articles\")\n",
    "        \n",
    "        # Show latest articles\n",
    "        print(f\"\\nLatest Articles:\")\n",
    "        latest = articles_df.nlargest(3, 'publication_date')\n",
    "        for idx, row in latest.iterrows():\n",
    "            print(f\"   - {row['title'][:70]}...\")\n",
    "            print(f\"     [{row['source_name']} - {row['category_name']} - {row['publication_date']}]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Dashboard creation failed: {e}\")\n",
    "\n",
    "# Create dashboard\n",
    "create_normalized_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f15207d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Tables created successfully\n",
      "STEP 2: Loaded 7856 articles from JSON\n",
      "\n",
      "First article structure:\n",
      "URL: https://dap-news.com/international/2022/03/29/232389/\n",
      "Source: https://dap-news.com\n",
      "Primary Category: 'MISSING'\n",
      "Tags: []\n",
      "Publication Date: 03/29/2022\n",
      "\n",
      "Found 100 articles without category in first 100\n",
      "Unique sources in first 100 articles: ['https://dap-news.com']\n",
      "Unique categories in first 100 articles: ['unknown']\n",
      "Sample tags in first 100 articles: []\n",
      "STEP 3: Starting data insertion...\n",
      "Step 3.1: Inserting sources...\n",
      "  Inserted source: dap-news.com\n",
      "Inserted 1 sources\n",
      "\n",
      "Step 3.2: Inserting categories...\n",
      "Inserted 1 categories\n",
      "\n",
      "Step 3.3: Inserting tags...\n",
      "Inserted 0 tags\n",
      "\n",
      "Step 3.4: Inserting articles and linking tags...\n",
      "  Processing article 0/7856...\n",
      "  Processing article 500/7856...\n",
      "  Processing article 1000/7856...\n",
      "  Processing article 1500/7856...\n",
      "  Processing article 2000/7856...\n",
      "  Processing article 2500/7856...\n",
      "  Processing article 3000/7856...\n",
      "  Processing article 3500/7856...\n",
      "  Processing article 4000/7856...\n",
      "  Processing article 4500/7856...\n",
      "  Processing article 5000/7856...\n",
      "  Processing article 5500/7856...\n",
      "  Processing article 6000/7856...\n",
      "  Processing article 6500/7856...\n",
      "  Processing article 7000/7856...\n",
      "  Processing article 7500/7856...\n",
      "Inserted 7856 articles\n",
      "Created 0 tag relationships\n",
      "Encountered 0 errors\n",
      "\n",
      "STEP 4: Verification Results\n",
      "==================================================\n",
      "Articles: 7856\n",
      "Sources: 1\n",
      "Categories: 1\n",
      "Tags: 0\n",
      "Article-Tag relationships: 0\n",
      "\n",
      "Sources:\n",
      "  1: dap-news.com (https://dap-news.com)\n",
      "\n",
      "Categories with article counts:\n",
      "  unknown: 7856 articles\n",
      "\n",
      "Top 10 tags:\n",
      "\n",
      "Sample articles with categories and tags:\n",
      "\n",
      "  Article 15:\n",
      "    Title: របស់រុស្ស៊ីថា ចារកម្មអ៊ុយក្រែន និងអង់គ្លេស ព្យាយាមសូកអ្នក បើ...\n",
      "    Category: unknown\n",
      "    URL: https://dap-news.com/international/2025/11/11/552432/\n",
      "    Tags: []\n",
      "\n",
      "  Article 229:\n",
      "    Title: មេអាវុធហត្ថខេត្តកំពង់ស្ពឺ ជម្រុញឱ្យមន្ត្រីជំនាញបង្កើនការចុះផ...\n",
      "    Category: unknown\n",
      "    URL: https://dap-news.com/national/2025/11/11/552395/\n",
      "    Tags: []\n",
      "\n",
      "  Article 345:\n",
      "    Title: មន្ទីរអភិវឌ្ឍន៍ជនបទ ប្រគល់ភារកិច្ចជូនអាជ្ញាធរភូមិឃុំនិងស្រុក...\n",
      "    Category: unknown\n",
      "    URL: https://dap-news.com/national/2025/11/11/552388/\n",
      "    Tags: []\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import psycopg2\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# %%\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'mates',\n",
    "    'user': 'postgres',\n",
    "    'password': '1',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "JSON_FILE_PATH = r\"d:\\Menghour\\MATES\\scraping\\notebook\\khmer_news_formatted.json\"\n",
    "\n",
    "# %%\n",
    "# STEP 1: Create normalized tables\n",
    "def create_tables():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Drop existing tables\n",
    "    cur.execute(\"DROP TABLE IF EXISTS article_tags CASCADE\")\n",
    "    cur.execute(\"DROP TABLE IF EXISTS articles CASCADE\") \n",
    "    cur.execute(\"DROP TABLE IF EXISTS tags CASCADE\")\n",
    "    cur.execute(\"DROP TABLE IF EXISTS sources CASCADE\")\n",
    "    cur.execute(\"DROP TABLE IF EXISTS categories CASCADE\")\n",
    "    \n",
    "    # Create categories table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE categories (\n",
    "            category_id SERIAL PRIMARY KEY,\n",
    "            category_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create sources table  \n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE sources (\n",
    "            source_id SERIAL PRIMARY KEY,\n",
    "            source_url VARCHAR(500) UNIQUE NOT NULL,\n",
    "            source_name VARCHAR(200) NOT NULL,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create tags table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE tags (\n",
    "            tag_id SERIAL PRIMARY KEY,\n",
    "            tag_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create articles table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE articles (\n",
    "            article_id SERIAL PRIMARY KEY,\n",
    "            url VARCHAR(1000) UNIQUE NOT NULL,\n",
    "            source_id INTEGER REFERENCES sources(source_id),\n",
    "            publication_date DATE NOT NULL,\n",
    "            scrape_date TIMESTAMP NOT NULL,\n",
    "            title TEXT NOT NULL,\n",
    "            content TEXT,\n",
    "            word_count INTEGER DEFAULT 0,\n",
    "            sentence_count INTEGER DEFAULT 0,\n",
    "            character_count INTEGER DEFAULT 0,\n",
    "            category_id INTEGER REFERENCES categories(category_id),\n",
    "            category_confidence DECIMAL(3,2) DEFAULT 0.0,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create article_tags junction table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE article_tags (\n",
    "            article_tag_id SERIAL PRIMARY KEY,\n",
    "            article_id INTEGER REFERENCES articles(article_id) ON DELETE CASCADE,\n",
    "            tag_id INTEGER REFERENCES tags(tag_id) ON DELETE CASCADE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(article_id, tag_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"STEP 1: Tables created successfully\")\n",
    "\n",
    "create_tables()\n",
    "\n",
    "# %%\n",
    "# STEP 2: Load and inspect JSON data with error handling\n",
    "def inspect_data():\n",
    "    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    print(f\"STEP 2: Loaded {len(articles)} articles from JSON\")\n",
    "    \n",
    "    # Check first article structure with safe access\n",
    "    first_article = articles[0]\n",
    "    print(\"\\nFirst article structure:\")\n",
    "    print(f\"URL: {first_article.get('url', 'MISSING')}\")\n",
    "    print(f\"Source: {first_article.get('source', 'MISSING')}\")\n",
    "    print(f\"Primary Category: '{first_article.get('primary_category', 'MISSING')}'\")\n",
    "    print(f\"Tags: {first_article.get('tags', [])}\")\n",
    "    print(f\"Publication Date: {first_article.get('publication_date', 'MISSING')}\")\n",
    "    \n",
    "    # Collect unique values with safe access\n",
    "    sources = set()\n",
    "    categories = set()\n",
    "    all_tags = set()\n",
    "    \n",
    "    articles_without_category = 0\n",
    "    \n",
    "    for article in articles[:100]:  # Check first 100 articles\n",
    "        # Source\n",
    "        if article.get('source'):\n",
    "            sources.add(article['source'])\n",
    "        \n",
    "        # Category - handle missing categories\n",
    "        category = article.get('primary_category')\n",
    "        if category:\n",
    "            categories.add(category)\n",
    "        else:\n",
    "            articles_without_category += 1\n",
    "            categories.add('unknown')  # Add default category\n",
    "        \n",
    "        # Tags\n",
    "        for tag in article.get('tags', []):\n",
    "            if tag:\n",
    "                all_tags.add(tag)\n",
    "    \n",
    "    print(f\"\\nFound {articles_without_category} articles without category in first 100\")\n",
    "    print(f\"Unique sources in first 100 articles: {list(sources)}\")\n",
    "    print(f\"Unique categories in first 100 articles: {list(categories)}\")\n",
    "    print(f\"Sample tags in first 100 articles: {list(all_tags)[:10]}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "articles_data = inspect_data()\n",
    "\n",
    "# %%\n",
    "# STEP 3: Insert data in logical order\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"STEP 3: Starting data insertion...\")\n",
    "\n",
    "# Step 3.1: Insert sources\n",
    "print(\"Step 3.1: Inserting sources...\")\n",
    "sources_inserted = set()\n",
    "\n",
    "for article in articles_data:\n",
    "    source_url = article.get('source')\n",
    "    if source_url and source_url not in sources_inserted:\n",
    "        source_name = source_url.replace('https://', '').replace('http://', '').split('/')[0]\n",
    "        try:\n",
    "            cur.execute(\n",
    "                \"INSERT INTO sources (source_url, source_name) VALUES (%s, %s)\",\n",
    "                (source_url, source_name)\n",
    "            )\n",
    "            sources_inserted.add(source_url)\n",
    "            print(f\"  Inserted source: {source_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Source already exists: {source_name}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(sources_inserted)} sources\")\n",
    "\n",
    "# Step 3.2: Insert categories (including 'unknown' for missing categories)\n",
    "print(\"\\nStep 3.2: Inserting categories...\")\n",
    "categories_inserted = set()\n",
    "\n",
    "# First, ensure 'unknown' category exists for articles without category\n",
    "try:\n",
    "    cur.execute(\"INSERT INTO categories (category_name) VALUES ('unknown') ON CONFLICT DO NOTHING\")\n",
    "    categories_inserted.add('unknown')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for article in articles_data:\n",
    "    category_name = article.get('primary_category', 'unknown')\n",
    "    if category_name and category_name not in categories_inserted:\n",
    "        try:\n",
    "            cur.execute(\n",
    "                \"INSERT INTO categories (category_name) VALUES (%s) ON CONFLICT DO NOTHING\",\n",
    "                (category_name,)\n",
    "            )\n",
    "            categories_inserted.add(category_name)\n",
    "            print(f\"  Inserted category: {category_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Category already exists: {category_name}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(categories_inserted)} categories\")\n",
    "\n",
    "# Step 3.3: Insert tags\n",
    "print(\"\\nStep 3.3: Inserting tags...\")\n",
    "tags_inserted = set()\n",
    "\n",
    "for article in articles_data:\n",
    "    for tag_name in article.get('tags', []):\n",
    "        if tag_name and tag_name not in tags_inserted:\n",
    "            try:\n",
    "                cur.execute(\n",
    "                    \"INSERT INTO tags (tag_name) VALUES (%s) ON CONFLICT DO NOTHING\",\n",
    "                    (tag_name,)\n",
    "                )\n",
    "                tags_inserted.add(tag_name)\n",
    "            except Exception as e:\n",
    "                pass  # Tag already exists\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(tags_inserted)} tags\")\n",
    "\n",
    "# Step 3.4: Insert articles and link tags\n",
    "print(\"\\nStep 3.4: Inserting articles and linking tags...\")\n",
    "articles_inserted = 0\n",
    "tags_linked = 0\n",
    "errors = 0\n",
    "\n",
    "for i, article in enumerate(articles_data):\n",
    "    if i % 500 == 0:\n",
    "        print(f\"  Processing article {i}/{len(articles_data)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get source_id\n",
    "        source_url = article.get('source')\n",
    "        if not source_url:\n",
    "            errors += 1\n",
    "            continue\n",
    "            \n",
    "        cur.execute(\"SELECT source_id FROM sources WHERE source_url = %s\", (source_url,))\n",
    "        source_result = cur.fetchone()\n",
    "        if not source_result:\n",
    "            errors += 1\n",
    "            continue\n",
    "        source_id = source_result[0]\n",
    "        \n",
    "        # Get category_id - handle missing categories  \n",
    "        category_name = article.get('primary_category', 'unknown')\n",
    "        cur.execute(\"SELECT category_id FROM categories WHERE category_name = %s\", (category_name,))\n",
    "        category_result = cur.fetchone()\n",
    "        category_id = category_result[0] if category_result else None\n",
    "        \n",
    "        # Convert dates\n",
    "        pub_date_str = article.get('publication_date')\n",
    "        scrape_date_str = article.get('scrape_date')\n",
    "        if not pub_date_str or not scrape_date_str:\n",
    "            errors += 1\n",
    "            continue\n",
    "            \n",
    "        pub_date = datetime.strptime(pub_date_str, '%m/%d/%Y').date()\n",
    "        scrape_date = datetime.strptime(scrape_date_str, '%m/%d/%Y %H:%M')\n",
    "        \n",
    "        # Insert article\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO articles (\n",
    "                url, source_id, publication_date, scrape_date, title, content,\n",
    "                word_count, sentence_count, character_count, category_id, category_confidence\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (url) DO NOTHING\n",
    "            RETURNING article_id\n",
    "        \"\"\", (\n",
    "            article.get('url'), source_id, pub_date, scrape_date, article.get('title', ''),\n",
    "            article.get('content', ''), article.get('word_count', 0), \n",
    "            article.get('sentence_count', 0), article.get('character_count', 0),\n",
    "            category_id, article.get('category_confidence', 0.0)\n",
    "        ))\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        if result:\n",
    "            article_id = result[0]\n",
    "            articles_inserted += 1\n",
    "            \n",
    "            # Link tags to this article\n",
    "            for tag_name in article.get('tags', []):\n",
    "                if tag_name:\n",
    "                    cur.execute(\"SELECT tag_id FROM tags WHERE tag_name = %s\", (tag_name,))\n",
    "                    tag_result = cur.fetchone()\n",
    "                    if tag_result:\n",
    "                        tag_id = tag_result[0]\n",
    "                        try:\n",
    "                            cur.execute(\n",
    "                                \"INSERT INTO article_tags (article_id, tag_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                                (article_id, tag_id)\n",
    "                            )\n",
    "                            tags_linked += 1\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            conn.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors < 10:  # Only print first 10 errors\n",
    "            print(f\"Error processing article {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {articles_inserted} articles\")\n",
    "print(f\"Created {tags_linked} tag relationships\")\n",
    "print(f\"Encountered {errors} errors\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# %%\n",
    "# STEP 4: Verify the results\n",
    "def verify_results():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    print(\"\\nSTEP 4: Verification Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Count records in each table\n",
    "    cur.execute(\"SELECT COUNT(*) FROM articles\")\n",
    "    articles_count = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(*) FROM sources\") \n",
    "    sources_count = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(*) FROM categories\")\n",
    "    categories_count = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(*) FROM tags\")\n",
    "    tags_count = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(*) FROM article_tags\")\n",
    "    article_tags_count = cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"Articles: {articles_count}\")\n",
    "    print(f\"Sources: {sources_count}\")\n",
    "    print(f\"Categories: {categories_count}\") \n",
    "    print(f\"Tags: {tags_count}\")\n",
    "    print(f\"Article-Tag relationships: {article_tags_count}\")\n",
    "    \n",
    "    # Show sources\n",
    "    print(\"\\nSources:\")\n",
    "    cur.execute(\"SELECT source_id, source_url, source_name FROM sources\")\n",
    "    for source_id, source_url, source_name in cur.fetchall():\n",
    "        print(f\"  {source_id}: {source_name} ({source_url})\")\n",
    "    \n",
    "    # Show categories with article counts\n",
    "    print(\"\\nCategories with article counts:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT c.category_name, COUNT(a.article_id) \n",
    "        FROM categories c \n",
    "        LEFT JOIN articles a ON c.category_id = a.category_id \n",
    "        GROUP BY c.category_name \n",
    "        ORDER BY COUNT(a.article_id) DESC\n",
    "    \"\"\")\n",
    "    for category_name, count in cur.fetchall():\n",
    "        print(f\"  {category_name}: {count} articles\")\n",
    "    \n",
    "    # Show top tags\n",
    "    print(\"\\nTop 10 tags:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT t.tag_name, COUNT(at.article_id) \n",
    "        FROM tags t \n",
    "        JOIN article_tags at ON t.tag_id = at.tag_id \n",
    "        GROUP BY t.tag_name \n",
    "        ORDER BY COUNT(at.article_id) DESC \n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    for tag_name, count in cur.fetchall():\n",
    "        print(f\"  {tag_name}: {count} articles\")\n",
    "    \n",
    "    # Show sample articles with their categories and tags\n",
    "    print(\"\\nSample articles with categories and tags:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT a.article_id, a.title, c.category_name, a.url\n",
    "        FROM articles a\n",
    "        LEFT JOIN categories c ON a.category_id = c.category_id\n",
    "        ORDER BY a.publication_date DESC\n",
    "        LIMIT 3\n",
    "    \"\"\")\n",
    "    for article_id, title, category, url in cur.fetchall():\n",
    "        print(f\"\\n  Article {article_id}:\")\n",
    "        print(f\"    Title: {title[:60]}...\")\n",
    "        print(f\"    Category: {category}\")\n",
    "        print(f\"    URL: {url}\")\n",
    "        \n",
    "        # Get tags for this article\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT t.tag_name \n",
    "            FROM tags t\n",
    "            JOIN article_tags at ON t.tag_id = at.tag_id\n",
    "            WHERE at.article_id = %s\n",
    "        \"\"\", (article_id,))\n",
    "        tags = [tag[0] for tag in cur.fetchall()]\n",
    "        print(f\"    Tags: {tags}\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "verify_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_v3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
